{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in what manner t\n",
      "['i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said', 'to', 'me']\n",
      "Total Tokens: 118682\n",
      "Unique Tokens: 7409\n",
      "Total Sequences: 118631\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, 'r')\n",
    "\ttext = file.read()\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\ttokens = doc.split()\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens\n",
    "\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "in_filename = 'republic_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\tseq = tokens[i-length:i]\n",
    "\tline = ' '.join(seq)\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            370500    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7410)              748410    \n",
      "=================================================================\n",
      "Total params: 1,269,810\n",
      "Trainable params: 1,269,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, 'r')\n",
    "\ttext = file.read()\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 118631 samples\n",
      "Epoch 1/20\n",
      "118631/118631 [==============================] - 86s 723us/sample - loss: 3.8491 - accuracy: 0.2293\n",
      "Epoch 2/20\n",
      "118631/118631 [==============================] - 85s 715us/sample - loss: 3.8197 - accuracy: 0.2330\n",
      "Epoch 3/20\n",
      "118631/118631 [==============================] - 85s 719us/sample - loss: 3.7913 - accuracy: 0.2358\n",
      "Epoch 4/20\n",
      "118631/118631 [==============================] - 85s 718us/sample - loss: 3.7628 - accuracy: 0.2381\n",
      "Epoch 5/20\n",
      "118631/118631 [==============================] - 85s 718us/sample - loss: 3.7359 - accuracy: 0.2425\n",
      "Epoch 6/20\n",
      "118631/118631 [==============================] - 86s 722us/sample - loss: 3.7083 - accuracy: 0.2450\n",
      "Epoch 7/20\n",
      "118631/118631 [==============================] - 85s 716us/sample - loss: 3.6816 - accuracy: 0.2474\n",
      "Epoch 8/20\n",
      "118631/118631 [==============================] - 86s 721us/sample - loss: 3.6544 - accuracy: 0.2510\n",
      "Epoch 9/20\n",
      "118631/118631 [==============================] - 86s 723us/sample - loss: 3.6317 - accuracy: 0.2535\n",
      "Epoch 10/20\n",
      "118631/118631 [==============================] - 86s 724us/sample - loss: 3.6063 - accuracy: 0.2574\n",
      "Epoch 11/20\n",
      "118631/118631 [==============================] - 86s 724us/sample - loss: 3.5821 - accuracy: 0.2598\n",
      "Epoch 12/20\n",
      "118631/118631 [==============================] - 86s 726us/sample - loss: 3.5594 - accuracy: 0.2628\n",
      "Epoch 13/20\n",
      "118631/118631 [==============================] - 87s 729us/sample - loss: 3.5354 - accuracy: 0.2660\n",
      "Epoch 14/20\n",
      "118631/118631 [==============================] - 84s 710us/sample - loss: 3.5117 - accuracy: 0.2686\n",
      "Epoch 15/20\n",
      "118631/118631 [==============================] - 85s 713us/sample - loss: 3.4892 - accuracy: 0.2708\n",
      "Epoch 16/20\n",
      "118631/118631 [==============================] - 84s 709us/sample - loss: 3.4692 - accuracy: 0.2745\n",
      "Epoch 17/20\n",
      "118631/118631 [==============================] - 84s 710us/sample - loss: 3.4466 - accuracy: 0.2777\n",
      "Epoch 18/20\n",
      "118631/118631 [==============================] - 85s 713us/sample - loss: 3.4244 - accuracy: 0.2808\n",
      "Epoch 19/20\n",
      "118631/118631 [==============================] - 85s 719us/sample - loss: 3.4029 - accuracy: 0.2830\n",
      "Epoch 20/20\n",
      "118631/118631 [==============================] - 86s 726us/sample - loss: 3.3806 - accuracy: 0.2856\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=20)\n",
    "\n",
    "model.save('model.h5')\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and use of women and children is to follow the path on which we originally started when we said that the men were to be the guardians and watchdogs of the herd true let us further suppose the birth and education of our women to be subject to similar or nearly\n",
      "\n",
      "GENER\n",
      "conceits that is the same principle and borne by the same time and the other virtues is the other of all sorts of materials and assemblies and the other virtues of the state and the echo of the soul and the other of the stronger and the other of the\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, 'r')\n",
    "\ttext = file.read()\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\tfor _ in range(n_words):\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    "\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "print(\"Generated\")\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
