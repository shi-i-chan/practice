{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secure-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "oriented-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smac_params(start_ma_w: int, end_ma_w: int, step: int):\n",
    "    params = []\n",
    "    for short_ma in range(start_ma_w, end_ma_w, step):\n",
    "        for long_ma in range(short_ma + 1, end_ma_w + 1, step):\n",
    "            params.append(str((short_ma, long_ma)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "regulated-neighbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1997001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_smac_params(2, 2000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "noted-radiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1998"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(range(2, 2000, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset.\n",
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "  'path/to/main_directory', batch_size=64, image_size=(200, 200))\n",
    "\n",
    "# For demonstration, iterate over the batches yielded by the dataset.\n",
    "for data, labels in dataset:\n",
    "    print(data.shape)  # (64, 200, 200, 3)\n",
    "    print(data.dtype)  # float32\n",
    "    print(labels.shape)  # (64,)\n",
    "    print(labels.dtype)  # int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = keras.preprocessing.text_dataset_from_directory(\n",
    "  'path/to/main_directory', batch_size=64)\n",
    "\n",
    "# For demonstration, iterate over the batches yielded by the dataset.\n",
    "for data, labels in dataset:\n",
    "    print(data.shape)  # (64,)\n",
    "    print(data.dtype)  # string\n",
    "    print(labels.shape)  # (64,)\n",
    "    print(labels.dtype)  # int32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-nepal",
   "metadata": {},
   "source": [
    "### Example: turning strings into sequences of integer word indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Example training data, of dtype `string`.\n",
    "training_data = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]])\n",
    "\n",
    "# Create a TextVectorization layer instance. It can be configured to either\n",
    "# return integer token indices, or a dense token representation (e.g. multi-hot\n",
    "# or TF-IDF). The text standardization and text splitting algorithms are fully\n",
    "# configurable.\n",
    "vectorizer = TextVectorization(output_mode=\"int\")\n",
    "\n",
    "# Calling `adapt` on an array or dataset makes the layer generate a vocabulary\n",
    "# index for the data, which can then be reused when seeing new data.\n",
    "vectorizer.adapt(training_data)\n",
    "\n",
    "# After calling adapt, the layer is able to encode any n-gram it has seen before\n",
    "# in the `adapt()` data. Unknown n-grams are encoded via an \"out-of-vocabulary\"\n",
    "# token.\n",
    "integer_data = vectorizer(training_data)\n",
    "print(integer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-wrist",
   "metadata": {},
   "source": [
    "### Example: turning strings into sequences of one-hot encoded bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Example training data, of dtype `string`.\n",
    "training_data = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]])\n",
    "\n",
    "# Create a TextVectorization layer instance. It can be configured to either\n",
    "# return integer token indices, or a dense token representation (e.g. multi-hot\n",
    "# or TF-IDF). The text standardization and text splitting algorithms are fully\n",
    "# configurable.\n",
    "vectorizer = TextVectorization(output_mode=\"binary\", ngrams=2)\n",
    "\n",
    "# Calling `adapt` on an array or dataset makes the layer generate a vocabulary\n",
    "# index for the data, which can then be reused when seeing new data.\n",
    "vectorizer.adapt(training_data)\n",
    "\n",
    "# After calling adapt, the layer is able to encode any n-gram it has seen before\n",
    "# in the `adapt()` data. Unknown n-grams are encoded via an \"out-of-vocabulary\"\n",
    "# token.\n",
    "integer_data = vectorizer(training_data)\n",
    "print(integer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-warehouse",
   "metadata": {},
   "source": [
    "### Example: normalizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Normalization\n",
    "\n",
    "# Example image data, with values in the [0, 255] range\n",
    "training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "\n",
    "normalizer = Normalization(axis=-1)\n",
    "normalizer.adapt(training_data)\n",
    "\n",
    "normalized_data = normalizer(training_data)\n",
    "print(\"var: %.4f\" % np.var(normalized_data))\n",
    "print(\"mean: %.4f\" % np.mean(normalized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-muscle",
   "metadata": {},
   "source": [
    "### Example: rescaling & center-cropping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amazing-salem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (64, 150, 150, 3)\n",
      "min: 0.0\n",
      "max: 1.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import CenterCrop\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "\n",
    "# Example image data, with values in the [0, 255] range\n",
    "training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "\n",
    "cropper = CenterCrop(height=150, width=150)\n",
    "scaler = Rescaling(scale=1.0 / 255)\n",
    "\n",
    "output_data = scaler(cropper(training_data))\n",
    "print(\"shape:\", output_data.shape)\n",
    "print(\"min:\", np.min(output_data))\n",
    "print(\"max:\", np.max(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-chamber",
   "metadata": {},
   "source": [
    "### Building models with the Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "saved-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we expect our inputs to be RGB images of arbitrary size\n",
    "inputs = keras.Input(shape=(None, None, 3))\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Center-crop images to 150x150\n",
    "x = CenterCrop(height=150, width=150)(inputs)\n",
    "# Rescale images to [0, 1]\n",
    "x = Rescaling(scale=1.0 / 255)(x)\n",
    "\n",
    "# Apply some convolution and pooling layers\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "\n",
    "# Apply global average pooling to get flat feature vectors\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a dense classifier on top\n",
    "num_classes = 10\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "organizational-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "harmful-garlic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "processed_data = model(data)\n",
    "print(processed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "metric-classics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " center_crop_1 (CenterCrop)  (None, 150, 150, 3)       0         \n",
      "                                                                 \n",
      " rescaling_1 (Rescaling)     (None, 150, 150, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 49, 49, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 47, 47, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 32)        9248      \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 32)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,722\n",
      "Trainable params: 19,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-shoot",
   "metadata": {},
   "source": [
    "### Training models with fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(numpy_array_of_samples, numpy_array_of_labels,\n",
    "          batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or\n",
    "model.fit(dataset_of_samples_and_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data as Numpy arrays\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Build a simple model\n",
    "inputs = keras.Input(shape=(28, 28))\n",
    "x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# Train the model for 1 epoch from Numpy data\n",
    "batch_size = 64\n",
    "print(\"Fit on NumPy data\")\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=1)\n",
    "\n",
    "# Train the model for 1 epoch using a dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "print(\"Fit on Dataset\")\n",
    "history = model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can pass a list of metric objects to compile(), like this:\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "history = model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='path/to/my/model_{epoch}.h5',\n",
    "        save_freq='epoch')\n",
    "]\n",
    "model.fit(dataset, epochs=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-tractor",
   "metadata": {},
   "source": [
    "### Monitoring training progress with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "model.fit(dataset, epochs=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(val_dataset)  # returns loss and metrics\n",
    "print(\"loss: %.2f\" % loss)\n",
    "print(\"acc: %.2f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_dataset)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-yemen",
   "metadata": {},
   "source": [
    "### Using fit() with a custom training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a simple example that reimplements what fit() normally does:\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "    # Unpack the data. Its structure depends on your model and\n",
    "    # on what you pass to `fit()`.\n",
    "    x, y = data\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = self(x, training=True)  # Forward pass\n",
    "        # Compute the loss value\n",
    "        # (the loss function is configured in `compile()`)\n",
    "        loss = self.compiled_loss(y, y_pred,\n",
    "                                regularization_losses=self.losses)\n",
    "    # Compute gradients\n",
    "    trainable_vars = self.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    # Update weights\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    # Update metrics (includes the metric that tracks the loss)\n",
    "    self.compiled_metrics.update_state(y, y_pred)\n",
    "    # Return a dict mapping metric names to current value\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# Construct and compile an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[...])\n",
    "\n",
    "# Just use `fit` as usual\n",
    "model.fit(dataset, epochs=3, callbacks=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-merit",
   "metadata": {},
   "source": [
    "### Debugging your model with eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-tobago",
   "metadata": {},
   "source": [
    "### Speeding up training with multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = Model(...)\n",
    "    model.compile(...)\n",
    "\n",
    "# Train the model on all available devices.\n",
    "train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n",
    "\n",
    "# Test the model on all available devices.\n",
    "model.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-finding",
   "metadata": {},
   "source": [
    "### Doing preprocessing synchronously on-device vs. asynchronously on host CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training data, of dtype `string`.\n",
    "samples = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]])\n",
    "labels = [[0], [1]]\n",
    "\n",
    "# Prepare a TextVectorization layer.\n",
    "vectorizer = TextVectorization(output_mode=\"int\")\n",
    "vectorizer.adapt(samples)\n",
    "\n",
    "# Asynchronous preprocessing: the text vectorization is part of the tf.data pipeline.\n",
    "# First, create a dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((samples, labels)).batch(2)\n",
    "# Apply text vectorization to the samples\n",
    "dataset = dataset.map(lambda x, y: (vectorizer(x), y))\n",
    "# Prefetch with a buffer size of 2 batches\n",
    "dataset = dataset.prefetch(2)\n",
    "\n",
    "# Our model should expect sequences of integers as inputs\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(input_dim=10, output_dim=32)(inputs)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", run_eagerly=True)\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare this to doing text vectorization as part of the model:\n",
    "# Our dataset will yield samples that are strings\n",
    "dataset = tf.data.Dataset.from_tensor_slices((samples, labels)).batch(2)\n",
    "\n",
    "# Our model should expect strings as inputs\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectorizer(inputs)\n",
    "x = layers.Embedding(input_dim=10, output_dim=32)(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", run_eagerly=True)\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype='string')\n",
    "x = vectorizer(inputs)\n",
    "outputs = trained_model(x)\n",
    "end_to_end_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-encounter",
   "metadata": {},
   "source": [
    "### Finding the best model configuration with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    x = layers.Dense(\n",
    "        units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "        activation='relu')(inputs)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "\n",
    "tuner = keras_tuner.tuners.Hyperband(\n",
    "  build_model,\n",
    "  objective='val_loss',\n",
    "  max_epochs=100,\n",
    "  max_trials=200,\n",
    "  executions_per_trial=2,\n",
    "  directory='my_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(dataset, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-baptist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-drilling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
